{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import necesseray libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions as per the instructions\n",
    "def linear_function(x):\n",
    "    return 3 * x + 1\n",
    "\n",
    "def quadratic_function(x):\n",
    "    return 5 * x**2 + 3 * x + 1\n",
    "\n",
    "def cubic_function(x):\n",
    "    return 7 * x**3 + 5 * x**2 + 3 * x + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "def denormalize(data, original_mean, original_std):\n",
    "    return data * original_std + original_mean\n",
    "\n",
    "# Data generation function\n",
    "def data_generation(function, batch_s=256):\n",
    "    data = []\n",
    "    x = 20 * np.random.randn(batch_s)  # Random inputs\n",
    "    for i in range(batch_s):\n",
    "        y = function(x[i])\n",
    "        data.append([x[i], y])  # Append to the list\n",
    "\n",
    "    data = torch.FloatTensor(data)\n",
    "    x_min, x_max = data[:, 0].min(), data[:, 0].max()\n",
    "    y_min, y_max = data[:, 1].min(), data[:, 1].max()\n",
    "    data[:, 0] = normalize(data[:, 0])\n",
    "    data[:, 1] = normalize(data[:, 1])\n",
    "    return data, x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        combined = torch.cat((z, x), dim=1)\n",
    "        return self.model(combined)\n",
    "\n",
    "# Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_gan(generator, discriminator, data_function, epochs=10000, batch_size=256, lr=0.0002):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Generate real data\n",
    "        real_data, x_min, x_max, y_min, y_max = data_generation(data_function, batch_size)\n",
    "        x_values = real_data[:, 0].unsqueeze(1)\n",
    "\n",
    "        # Generate fake data\n",
    "        fake_noise = torch.randn(batch_size, 10)  # Latent dimension\n",
    "        y_fake = generator(fake_noise, x_values)\n",
    "        fake_data = torch.cat((x_values, y_fake), dim=1)\n",
    "\n",
    "        # Labels for real and fake\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        # Train discriminator\n",
    "        d_loss_real = criterion(discriminator(real_data), real_labels)\n",
    "        d_loss_fake = criterion(discriminator(fake_data), fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        fake_noise = torch.randn(batch_size, 10)\n",
    "        y_fake = generator(fake_noise, x_values)\n",
    "        fake_data = torch.cat((x_values, y_fake), dim=1)\n",
    "        g_loss = criterion(discriminator(fake_data), real_labels)\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print losses\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "normalize() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m linear_gen \u001b[38;5;241m=\u001b[39m Generator(noise_dim, x_dim, output_dim)\n\u001b[1;32m     32\u001b[0m linear_disc \u001b[38;5;241m=\u001b[39m Discriminator(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m plot_results(linear_gen, linear_function, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear Function: y = 3x + 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Quadratic\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, data_function, epochs, batch_size, lr, latent_dim)\u001b[0m\n\u001b[1;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSmoothL1Loss()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Generate real data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     real_data, x_min, x_max, y_min, y_max \u001b[38;5;241m=\u001b[39m \u001b[43mdata_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x_values \u001b[38;5;241m=\u001b[39m real_data[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Generate fake data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m, in \u001b[0;36mdata_generation\u001b[0;34m(function, batch_s)\u001b[0m\n\u001b[1;32m     19\u001b[0m y_min, y_max \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(), data[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Normalize data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m data[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m data[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m normalize(data[:, \u001b[38;5;241m1\u001b[39m], y_min, y_max)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, x_min, x_max, y_min, y_max\n",
      "\u001b[0;31mTypeError\u001b[0m: normalize() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "def plot_results(generator, data_function, function_name):\n",
    "    # Generate 2500 real samples\n",
    "    real_data, x_min, x_max, y_min, y_max = data_generation(data_function, batch_s=2500)\n",
    "    x_real = denormalize(real_data[:, 0], x_min, x_max)\n",
    "    y_real = denormalize(real_data[:, 1], y_min, y_max)\n",
    "\n",
    "    # Generate 2500 fake samples\n",
    "    fake_noise = torch.randn(2500, 10)  # Latent dimension\n",
    "    x_fake = torch.linspace(-1, 1, 2500).unsqueeze(1)\n",
    "    combined_input = torch.cat((fake_noise, x_fake), dim=1)\n",
    "    y_fake = generator(fake_noise, x_fake).detach()\n",
    "    x_fake = denormalize(x_fake, x_min, x_max)\n",
    "    y_fake = denormalize(y_fake, y_min, y_max)\n",
    "\n",
    "    # Plot real and fake data\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x_real.numpy(), y_real.numpy(), label=\"Original Data (2500)\", alpha=0.6)\n",
    "    plt.scatter(x_fake.numpy(), y_fake.numpy(), label=\"Generated Data (2500)\", alpha=0.6)\n",
    "    plt.title(function_name)\n",
    "    plt.xlabel(\"Inputs\")\n",
    "    plt.ylabel(\"Outputs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "# Parameters\n",
    "noise_dim = 10\n",
    "x_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "# Linear\n",
    "print(\"Training GAN for Linear Data...\")\n",
    "linear_gen = Generator(noise_dim, x_dim, output_dim)\n",
    "linear_disc = Discriminator(2)\n",
    "train_gan(linear_gen, linear_disc, linear_function, epochs=10000)\n",
    "plot_results(linear_gen, linear_function, \"Linear Function: y = 3x + 1\")\n",
    "\n",
    "# Quadratic\n",
    "print(\"Training GAN for Quadratic Data...\")\n",
    "quadratic_gen = Generator(noise_dim, x_dim, output_dim)\n",
    "quadratic_disc = Discriminator(2)\n",
    "train_gan(quadratic_gen, quadratic_disc, quadratic_function, epochs=10000)\n",
    "plot_results(quadratic_gen, quadratic_function, \"Quadratic Function: y = 5x^2 + 3x + 1\")\n",
    "\n",
    "# Cubic\n",
    "print(\"Training GAN for Cubic Data...\")\n",
    "cubic_gen = Generator(noise_dim, x_dim, output_dim)\n",
    "cubic_disc = Discriminator(2)\n",
    "train_gan(cubic_gen, cubic_disc, cubic_function, epochs=15000)\n",
    "plot_results(cubic_gen, cubic_function, \"Cubic Function: y = 7x^3 + 5x^2 + 3x + 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
