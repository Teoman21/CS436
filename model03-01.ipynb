{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "#load the dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#trainset into 80% training and 20% validation\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_subset, val_subset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "#define batch_size\n",
    "batch_size = 16\n",
    "\n",
    "# data loaders for train validate and test\n",
    "trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#define classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Teoman/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /Users/Teoman/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# load vgg16 \n",
    "vgg16 = models.vgg16_bn(pretrained=True)  # Load VGG16 with batch normalization\n",
    "vgg16.classifier[6] = torch.nn.Linear(vgg16.classifier[6].in_features, num_classes)\n",
    "\n",
    "# load resnet18\n",
    "resnet18 = models.resnet18(pretrained=True)  #ResNet18\n",
    "resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "\n",
    "# load alexnet\n",
    "alexnet = models.alexnet(pretrained=True)  # Load AlexNet\n",
    "alexnet.classifier[6] = torch.nn.Linear(alexnet.classifier[6].in_features, num_classes)\n",
    "\n",
    "#put them as dictionary\n",
    "models_dict = {\n",
    "    \"VGG16\": vgg16,\n",
    "    \"ResNet18\": resnet18,\n",
    "    \"AlexNet\": alexnet\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the models\n",
    "def train_model(model, trainloader, valloader, num_epochs=5, lr=0.001):\n",
    "    cost = nn.CrossEntropyLoss() #use cross entropy as loss function\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9) #as optimizer using sgd\n",
    "    start_time = time.time() #start time for calculate the time of the model to train\n",
    "\n",
    "    model = model.to(device)  # Move model to GPU\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()  # models training model\n",
    "\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()  #zgradients as zero \n",
    "            outputs = model(inputs)  #forward pass\n",
    "            loss = cost(outputs, labels)  #compute the loss\n",
    "            loss.backward()  # backward pass\n",
    "            optimizer.step()  # update the weights\n",
    "\n",
    "            running_loss += loss.item() #all the loss here\n",
    "\n",
    "        # validaton through iterations(epochs)\n",
    "        val_accuracy = evaluate_model(model, valloader)\n",
    "        print(f\"epoch [{epoch+1}/{num_epochs}], loss: {running_loss/len(trainloader):.4f}, validation accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    training_time = time.time() - start_time #finish calculate time of model training\n",
    "    return training_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the models\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # model evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in dataloader: #iterate through dataloaders\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0) #calculate the total tp in these lines\n",
    "            correct += (predicted == labels).sum().item() \n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# calculate model size by parameters\n",
    "def calculate_model_size_in_memory(model):\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "#cross-validation function\n",
    "def cross_validate(model_class, model_name, trainset, num_folds=5, num_epochs=5):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "    #store teh accuracy time and model size in lists\n",
    "    accuracies = [] \n",
    "    training_times = []\n",
    "    model_sizes = []\n",
    "\n",
    "    print(f\"\\n=== Cross-Validation for {model_name} ===\") \n",
    "\n",
    "    #iterate through\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(trainset)):\n",
    "        print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        # data loaders for train and validaton dataseets\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        fold_trainloader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "        fold_valloader = DataLoader(trainset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        # Initialize a new model instance for each fold\n",
    "        model = model_class.to(device)  # Move model to GPU\n",
    "\n",
    "        # Train the model\n",
    "        training_time = train_model(model, fold_trainloader, fold_valloader, num_epochs=num_epochs)\n",
    "\n",
    "        # Evaluate model on the validation set\n",
    "        accuracy = evaluate_model(model, fold_valloader)\n",
    "        model_size = calculate_model_size_in_memory(model)\n",
    "\n",
    "        #put the result to their corresponding lists\n",
    "        accuracies.append(accuracy)\n",
    "        training_times.append(training_time)\n",
    "        model_sizes.append(model_size)\n",
    "\n",
    "        print(f\"Fold {fold + 1}: Accuracy = {accuracy:.2f}%, Time = {training_time:.2f} sec, Total Parameters: {model_size}\")\n",
    "\n",
    "    # calculate teh average of accuracy time and mode size\n",
    "    avg_accuracy = sum(accuracies) / num_folds\n",
    "    avg_time = sum(training_times) / num_folds\n",
    "    avg_size = sum(model_sizes) / num_folds\n",
    "\n",
    "    # print ht results\n",
    "    print(f\"\\nAverage Accuracy for {model_name}: {avg_accuracy:.2f}%\")\n",
    "    print(f\"Average Training Time: {avg_time:.2f} sec\")\n",
    "    print(f\"Average Model Size (Total Parameters): {avg_size}\")\n",
    "\n",
    "    return avg_accuracy, avg_time, avg_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cross-Validation for VGG16 ===\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# validate the each model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models_dict\u001b[38;5;241m.\u001b[39mitems(): \u001b[38;5;66;03m#take it from the models dctionary\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     avg_acc, avg_time, avg_size \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(model, model_name, trainset, num_folds, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#init the model and train\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model_copy \u001b[38;5;241m=\u001b[39m model  \u001b[38;5;66;03m# Ccopy for each fold the model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m training_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_trainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_valloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#eval on validaton set\u001b[39;00m\n\u001b[1;32m     25\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model_copy, fold_valloader)\n",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, trainloader, valloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m#forward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m cost(outputs, labels)  \u001b[38;5;66;03m#compute the loss\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# update the weights\u001b[39;00m\n\u001b[1;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m#all the loss here\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# validate the each model\n",
    "for model_name, model in models_dict.items(): #take it from the models dctionary\n",
    "    avg_acc, avg_time, avg_size = cross_validate(model, model_name, train_subset, num_folds=5, num_epochs=5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the best model\n",
    "best_model = max(models_dict, key=lambda name: evaluate_model(models_dict[name], valloader))\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "\n",
    "# eval the best model on the test dataset   \n",
    "test_accuracy = evaluate_model(models_dict[best_model], testloader)\n",
    "print(f\"Test Accuracy for Best Model ({best_model}): {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the confusion matrix on the best model\n",
    "\n",
    "def plot_confusion_matrix(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = torch.tensor([], dtype=torch.long)\n",
    "    all_labels = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    # get all the true labels\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:#iterate through dataloaders\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds = torch.cat((all_preds, preds), dim=0)\n",
    "            all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "\n",
    "    cm = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "\n",
    "    # plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {best_model}')\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = range(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"black\")\n",
    "\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for the best model\n",
    "plot_confusion_matrix(models_dict[best_model], testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Cross-Validation for VGG16 ===\n",
    "\n",
    "Fold 1/5\n",
    "epoch [1/5], loss: 1.0351, validation accuracy: 82.47%\n",
    "epoch [2/5], loss: 0.6056, validation accuracy: 84.84%\n",
    "epoch [3/5], loss: 0.4402, validation accuracy: 85.44%\n",
    "epoch [4/5], loss: 0.3368, validation accuracy: 85.28%\n",
    "epoch [5/5], loss: 0.2618, validation accuracy: 86.05%\n",
    "Fold 1: Accuracy = 86.05%, Time = 819.11 sec, Total Parameters: 134309962\n",
    "\n",
    "Fold 2/5\n",
    "epoch [1/5], loss: 0.2823, validation accuracy: 95.14%\n",
    "epoch [2/5], loss: 0.2034, validation accuracy: 94.99%\n",
    "epoch [3/5], loss: 0.1555, validation accuracy: 94.61%\n",
    "epoch [4/5], loss: 0.1415, validation accuracy: 93.69%\n",
    "epoch [5/5], loss: 0.1091, validation accuracy: 91.22%\n",
    "Fold 2: Accuracy = 91.22%, Time = 816.17 sec, Total Parameters: 134309962\n",
    "\n",
    "Fold 3/5\n",
    "epoch [1/5], loss: 0.1364, validation accuracy: 98.55%\n",
    "epoch [2/5], loss: 0.0998, validation accuracy: 98.04%\n",
    "epoch [3/5], loss: 0.0773, validation accuracy: 96.74%\n",
    "epoch [4/5], loss: 0.0715, validation accuracy: 97.69%\n",
    "epoch [5/5], loss: 0.0590, validation accuracy: 96.99%\n",
    "Fold 3: Accuracy = 96.99%, Time = 827.52 sec, Total Parameters: 134309962\n",
    "\n",
    "Fold 4/5\n",
    "epoch [1/5], loss: 0.0724, validation accuracy: 98.70%\n",
    "epoch [2/5], loss: 0.0576, validation accuracy: 99.10%\n",
    "epoch [3/5], loss: 0.0433, validation accuracy: 98.97%\n",
    "epoch [4/5], loss: 0.0468, validation accuracy: 98.46%\n",
    "epoch [5/5], loss: 0.0440, validation accuracy: 98.65%\n",
    "Fold 4: Accuracy = 98.65%, Time = 816.69 sec, Total Parameters: 134309962\n",
    "\n",
    "Fold 5/5\n",
    "epoch [1/5], loss: 0.0550, validation accuracy: 99.34%\n",
    "epoch [2/5], loss: 0.0365, validation accuracy: 99.45%\n",
    "epoch [3/5], loss: 0.0302, validation accuracy: 99.65%\n",
    "epoch [4/5], loss: 0.0288, validation accuracy: 98.94%\n",
    "epoch [5/5], loss: 0.0282, validation accuracy: 99.31%\n",
    "Fold 5: Accuracy = 99.31%, Time = 814.72 sec, Total Parameters: 134309962\n",
    "\n",
    "Average Accuracy for VGG16: 94.45%\n",
    "Average Training Time: 818.84 sec\n",
    "Average Model Size (Total Parameters): 134309962.0\n",
    "\n",
    "=== Cross-Validation for ResNet18 ===\n",
    "\n",
    "Fold 1/5\n",
    "epoch [1/5], loss: 1.5030, validation accuracy: 63.89%\n",
    "epoch [2/5], loss: 1.1138, validation accuracy: 71.20%\n",
    "epoch [3/5], loss: 0.9304, validation accuracy: 73.17%\n",
    "epoch [4/5], loss: 0.8335, validation accuracy: 75.38%\n",
    "epoch [5/5], loss: 0.7225, validation accuracy: 75.97%\n",
    "Fold 1: Accuracy = 75.97%, Time = 296.07 sec, Total Parameters: 11181642\n",
    "\n",
    "Fold 2/5\n",
    "epoch [1/5], loss: 0.7210, validation accuracy: 82.24%\n",
    "epoch [2/5], loss: 0.6193, validation accuracy: 81.00%\n",
    "epoch [3/5], loss: 0.5385, validation accuracy: 81.83%\n",
    "epoch [4/5], loss: 0.4804, validation accuracy: 82.56%\n",
    "epoch [5/5], loss: 0.4294, validation accuracy: 80.62%\n",
    "Fold 2: Accuracy = 80.62%, Time = 293.38 sec, Total Parameters: 11181642\n",
    "\n",
    "Fold 3/5\n",
    "epoch [1/5], loss: 0.4578, validation accuracy: 90.24%\n",
    "epoch [2/5], loss: 0.3932, validation accuracy: 89.70%\n",
    "epoch [3/5], loss: 0.3562, validation accuracy: 89.25%\n",
    "epoch [4/5], loss: 0.3263, validation accuracy: 87.95%\n",
    "epoch [5/5], loss: 0.2666, validation accuracy: 88.14%\n",
    "Fold 3: Accuracy = 88.14%, Time = 293.90 sec, Total Parameters: 11181642\n",
    "\n",
    "Fold 4/5\n",
    "epoch [1/5], loss: 0.3638, validation accuracy: 92.75%\n",
    "epoch [2/5], loss: 0.2705, validation accuracy: 90.90%\n",
    "epoch [3/5], loss: 0.2260, validation accuracy: 93.75%\n",
    "epoch [4/5], loss: 0.1955, validation accuracy: 91.95%\n",
    "epoch [5/5], loss: 0.1651, validation accuracy: 91.01%\n",
    "Fold 4: Accuracy = 91.01%, Time = 293.06 sec, Total Parameters: 11181642\n",
    "\n",
    "Fold 5/5\n",
    "epoch [1/5], loss: 0.2118, validation accuracy: 96.40%\n",
    "epoch [2/5], loss: 0.1595, validation accuracy: 94.99%\n",
    "epoch [3/5], loss: 0.1350, validation accuracy: 93.71%\n",
    "epoch [4/5], loss: 0.1331, validation accuracy: 95.71%\n",
    "epoch [5/5], loss: 0.1093, validation accuracy: 95.26%\n",
    "Fold 5: Accuracy = 95.26%, Time = 291.91 sec, Total Parameters: 11181642\n",
    "\n",
    "Average Accuracy for ResNet18: 86.20%\n",
    "Average Training Time: 293.66 sec\n",
    "Average Model Size (Total Parameters): 11181642.0\n",
    "\n",
    "=== Cross-Validation for AlexNet ===\n",
    "\n",
    "Fold 1/5\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-12-70d0800834a6> in <cell line: 2>()\n",
    "      1 # validate the each model\n",
    "      2 for model_name, model in models_dict.items(): #take it from the models dctionary\n",
    "----> 3     avg_acc, avg_time, avg_size = cross_validate(model, model_name, train_subset, num_folds=5, num_epochs=5)\n",
    "      4 \n",
    "\n",
    "12 frames\n",
    "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in _max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\n",
    "    828     if stride is None:\n",
    "    829         stride = torch.jit.annotate(List[int], [])\n",
    "--> 830     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
    "    831 \n",
    "    832 \n",
    "\n",
    "RuntimeError: Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
